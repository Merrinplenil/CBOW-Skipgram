# =================================================
# cosine_similarity_both_models.py
# Generates cosine similarity for CBOW and Skip-gram
# =================================================

import numpy as np
import math

# -------------------------------
# CONFIG
# -------------------------------
TOP_K = 5

QUERY_WORDS = [
    "login",
    "malware",
    "authentication",
    "encryption",
    "access"
]

EMBEDDING_FILES = {
    "cbow": "embeddings_cbow.csv",
    "skipgram": "embeddings_skipgram.csv"
}

# -------------------------------
# STEP 1: Load Vocabulary
# -------------------------------
id_to_word = {}

with open("vocab.txt", "r") as f:
    for line in f:
        word, idx, freq = line.strip().split(",")
        id_to_word[int(idx)] = word

# -------------------------------
# STEP 2: Cosine Similarity
# -------------------------------
def cosine_similarity(v1, v2):
    dot = np.dot(v1, v2)
    norm1 = math.sqrt(np.dot(v1, v1))
    norm2 = math.sqrt(np.dot(v2, v2))
    return dot / (norm1 * norm2 + 1e-8)

# -------------------------------
# STEP 3: Load Embeddings
# -------------------------------
def load_embeddings(file_name):
    embeddings = {}
    with open(file_name, "r") as f:
        next(f)  # skip header
        for line in f:
            parts = line.strip().split(",")
            word_id = int(parts[0])
            vector = np.array(list(map(float, parts[1:])))
            embeddings[word_id] = vector
    return embeddings

# -------------------------------
# STEP 4: Top-K Similar Words
# -------------------------------
def get_top_k(word, embeddings, k=TOP_K):
    query_id = None
    for idx, w in id_to_word.items():
        if w == word:
            query_id = idx
            break

    if query_id is None or query_id not in embeddings:
        return []

    query_vec = embeddings[query_id]
    similarities = []

    for idx, vec in embeddings.items():
        if idx != query_id:
            sim = cosine_similarity(query_vec, vec)
            similarities.append((id_to_word[idx], sim))

    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:k]

# -------------------------------
# STEP 5: Run for BOTH Models
# -------------------------------
for model_name, emb_file in EMBEDDING_FILES.items():

    print(f"\nProcessing {model_name.upper()} embeddings...")
    embeddings = load_embeddings(emb_file)

    output_file = f"similarity_results_{model_name}.txt"

    with open(output_file, "w") as f:
        f.write(f"Cosine Similarity Results ({model_name.upper()})\n")
        f.write("=" * 40 + "\n\n")

        for word in QUERY_WORDS:
            f.write(f"Query word: {word}\n")
            results = get_top_k(word, embeddings)

            for i, (w, s) in enumerate(results, start=1):
                f.write(f"{i}. {w} ({s:.4f})\n")

            f.write("\n")

    print(f"Saved results to {output_file}")

print("\nCosine similarity generation complete for BOTH models.")
